{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# MySQL connection parameters\n",
    "host= 'localhost'\n",
    "user= 'josh'\n",
    "password= 'go$T4GS'\n",
    "database= 'data_4999'\n",
    "\n",
    "# Create MySQL connection\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{user}:{password}@{host}/{database}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "import pandas as pd\n",
    "\n",
    "# SQL query to pull desired data for the most recent CostReport data for all Providers\n",
    "query = \"\"\"\n",
    "        SELECT t1.prov_id, p.overall_rating, t1.loc_type, t1.ownership, t1.snf_num_beds, t1.total_assets \n",
    "        FROM CostReports t1\n",
    "        JOIN (\n",
    "                SELECT prov_id, MAX(fiscal_end) AS max_fiscal_end\n",
    "                FROM CostReports\n",
    "                GROUP BY prov_id\n",
    "        ) t2 ON t1.prov_id = t2.prov_id AND t1.fiscal_end = t2.max_fiscal_end\n",
    "        LEFT JOIN (\n",
    "                SELECT DISTINCT prov_id, overall_rating\n",
    "                FROM ProviderInfo\n",
    "       ) p ON t1.prov_id = p.prov_id;\n",
    "        \"\"\"\n",
    "\n",
    "# Perform the query using the engine and load the results to a dataframe\n",
    "with engine.connect() as connection:\n",
    "    result = connection.execute(text(query))\n",
    "    df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Preprocessing\n",
    "categorical_cols = ['loc_type', 'ownership']\n",
    "quantitative_cols = ['snf_num_beds', 'total_assets', 'overall_rating']\n",
    "\n",
    "# Define imputers for different data types\n",
    "imputer_quantitative = SimpleImputer(strategy= 'median')  # Impute with median for quantitative data\n",
    "imputer_categorical = SimpleImputer(strategy= 'most_frequent')  # Impute with most frequent value for categorical data\n",
    "\n",
    "# Define preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer_categorical', imputer_categorical),\n",
    "            ('encoder', OneHotEncoder())\n",
    "        ]), categorical_cols),\n",
    "        ('quant', Pipeline([\n",
    "            ('imputer_quantitative', imputer_quantitative),\n",
    "            ('scaler', QuantileTransformer())\n",
    "        ]), quantitative_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Define a function to pass through TSNE transformation\n",
    "tsne_transformer = FunctionTransformer(func=lambda X: TSNE(n_components=2, init = 'random').fit_transform(X))\n",
    "\n",
    "# Clustering pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('tsne', tsne_transformer),  # for dimensionality reduction using t-SNE\n",
    "    ('clustering', AgglomerativeClustering())\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(df.drop(columns=['prov_id']))\n",
    "\n",
    "# Transform data and predict clusters\n",
    "transformed_data = pipeline.named_steps['tsne'].transform(pipeline.named_steps['preprocessor'].transform(df.drop(columns=['prov_id'])))\n",
    "clusters = pipeline.named_steps['clustering'].labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette = silhouette_score(transformed_data, clusters)\n",
    "\n",
    "# Calculate Davies-Bouldin index\n",
    "davies_bouldin = davies_bouldin_score(transformed_data, clusters)\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "df['cluster'] = clusters\n",
    "\n",
    "# Print clustering metrics\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot for clustering visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cluster in df['cluster'].unique():\n",
    "    plt.scatter(transformed_data[df['cluster'] == cluster, 0], transformed_data[df['cluster'] == cluster, 1], s= 5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_directory = 'Data 4999/BAC@MC 2024 Phase One Datasets/'\n",
    "\n",
    "with open(f'{data_directory}data-dictionary.json', 'r') as file:\n",
    "   data_dictionary = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zero(prov_id):\n",
    "\n",
    "    if len(prov_id) < 6:\n",
    "        return '0' + prov_id\n",
    "    \n",
    "    return prov_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rows(dataframe, folder_name):\n",
    "    \"\"\"\n",
    "    Clean DataFrame rows to remove observations where there are mostly NULL values.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): The DataFrame with rows to be cleand.\n",
    "        folder_name (str): The name of the folder which indicates what columns will be in this DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with cleaned rows.\n",
    "    \"\"\"\n",
    "    df = dataframe\n",
    "\n",
    "    match folder_name:\n",
    "\n",
    "        case 'CostReports':\n",
    "            # Drop rows where both fiscal_start and loc_type are NA\n",
    "            df = df.dropna(subset=['fiscal_start', 'loc_type'], how='all')\n",
    "            \n",
    "        case 'CovidVax':\n",
    "            # Drop rows where any observation in column 'pct_residents_primary_vax' is equal to 'not_available'\n",
    "            df = df[df['pct_residents_primary_vax'] != 'not available']\n",
    "\n",
    "        case 'HealthDeficiencies':\n",
    "            # Filter rows where the observation in column B has a length of 1\n",
    "            defstat_index = df.columns.get_loc('defstat')\n",
    "            mask = df['defstat'].str.len() == 1\n",
    "            \n",
    "            # Create a list of columns to shift\n",
    "            columns_to_shift = df.columns[defstat_index+1:].tolist()\n",
    "\n",
    "            # Convert values to object dtype before assigning them\n",
    "            shifted_values = df.loc[mask, columns_to_shift[1:]].astype(int).values\n",
    "\n",
    "            # Shift observations 1 to the left for rows where condition is met\n",
    "            df.loc[mask, columns_to_shift[:-1]] = shifted_values\n",
    "\n",
    "            # Drop the last column for rows where condition is met\n",
    "            df.loc[mask, columns_to_shift[-1]] = None\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_dataframe(dataframe, folder_name):\n",
    "    \"\"\"\n",
    "    Clean column names of a DataFrame using a lookup dictionary with regex patterns.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): The DataFrame with column names to be cleaned.\n",
    "        folder_name (str): The name of the folder which is being concatenated in the extract_transform function\n",
    "        \n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with cleaned column names.\n",
    "    \"\"\"\n",
    "    # Iterate over each column name\n",
    "    for column_name in dataframe.columns:\n",
    "        # Iterate over each regex pattern in the lookup dictionary\n",
    "        for replacement, pattern in data_dictionary[folder_name].items():\n",
    "            \n",
    "\n",
    "            # If the regex pattern matches the column name\n",
    "            if re.search(pattern, column_name.lower().strip()):\n",
    "\n",
    "                # Replace the column name with the replacement string\n",
    "                dataframe.rename(columns={column_name: replacement}, inplace=True)\n",
    "\n",
    "                break\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # Rename columns to drop for items not in the data dictionary\n",
    "            dataframe.rename(columns={column_name: 'DROPCOLUMN'}, inplace=True)\n",
    "\n",
    "    # Replace column names for 2021 CovidVax\n",
    "    if folder_name == 'CovidVax':\n",
    "        current_names = ['residents_2021','staff_2021']\n",
    "        new_names = ['pct_residents_primary_vax', 'pct_staff_primary_vax']\n",
    "    \n",
    "        for current_name, new_name in zip(current_names, new_names):\n",
    "            if current_name in dataframe.columns:\n",
    "                dataframe = dataframe.rename(columns={current_name: new_name})\n",
    "\n",
    "    # Drop columns with all NaN values\n",
    "    df = dataframe.dropna(axis=1, how='all')\n",
    "\n",
    "    # Clean rows\n",
    "    df = clean_rows(df, folder_name)\n",
    "\n",
    "    # Apply zeros for prov_id with less than 6 characters\n",
    "    df['prov_id'] = df['prov_id'].astype(str).apply(add_zero)\n",
    "\n",
    "    # Drop marked columns\n",
    "    df = df.drop(columns=[col for col in df.columns if col == 'DROPCOLUMN'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_transform(folder_path):\n",
    "    \"\"\"\n",
    "    Read folder contents for subdirectories containing .csv files. Read files ,csv into pandas DataFrames and then concatenate into master DataFrame by directory.\n",
    "    \n",
    "    Parameters:\n",
    "        folder_path (str): Path to directory containing sub directories with .csv files\n",
    "    \n",
    "    Returns:\n",
    "        List: List of pandas DataFrames stacked by directory\n",
    "    \"\"\"\n",
    "    #Initialize an empty dict to store the DataFrames\n",
    "    joined_dfs = {}\n",
    "\n",
    "    # Iterate over all folders in the directory\n",
    "    for folder_name in os.listdir(folder_path):\n",
    "        folder_full_path = os.path.join(folder_path, folder_name)\n",
    "        \n",
    "\n",
    "        #Check if the current item in directory is a folder\n",
    "        if os.path.isdir(folder_full_path):\n",
    "            # Initialize empty list to store DataFrames\n",
    "            dfs = []\n",
    "\n",
    "            # Iterate over every each CSV file in the folder\n",
    "            for file_name in os.listdir(folder_full_path):\n",
    "                \n",
    "                if file_name.endswith('.csv') :\n",
    "                    file_full_path = os.path.join(folder_full_path, file_name)\n",
    "\n",
    "                    # Read the CSV file into a DataFrame, use latin1 encoding\n",
    "                    df = pd.read_csv(file_full_path, encoding= 'latin1', low_memory= False)\n",
    "\n",
    "                    # Rename columns using clean columns function\n",
    "                    df = clean_dataframe(df, folder_name)\n",
    "                    \n",
    "                    # Append the DataFrame to the list\n",
    "                    dfs.append(df)       \n",
    "\n",
    "            # Stack all the dfs and convert strings to lowercase\n",
    "            folder_df = pd.concat(dfs, ignore_index= False).map(lambda x: x.lower() if isinstance(x, str) else x).reset_index()\n",
    "\n",
    "            # Store all folder_dfs in a dict\n",
    "            joined_dfs[folder_name] = folder_df\n",
    "    \n",
    "    return joined_dfs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataframe_to_mysql(df, table_name, engine, chunk_size= 50000):\n",
    "    try:\n",
    "        # Drop the 'index' column if it exists\n",
    "        df = df.drop(columns=['index'])\n",
    "    except KeyError:\n",
    "        pass  # 'index' column does not exist, continue without dropping\n",
    "\n",
    "    # Calculate number of chunks\n",
    "    num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size > 0 else 0)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(df))\n",
    "        df_chunk = df.iloc[start_idx:end_idx]\n",
    "\n",
    "        try:\n",
    "            # Upload chunk to MySQL table\n",
    "            df_chunk.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "            print(f'Chunk {i+1}/{num_chunks} uploaded successfully to MySQL table: {table_name}')\n",
    "        except Exception as e:\n",
    "            print(f'Error uploading chunk {i+1}/{num_chunks} to MySQL table: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/2 uploaded successfully to MySQL table: Penalties\n",
      "Chunk 2/2 uploaded successfully to MySQL table: Penalties\n",
      "Chunk 1/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 2/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 3/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 4/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 5/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 6/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 7/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 8/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 9/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 10/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 11/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 12/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 13/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 14/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 15/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 16/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 17/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 18/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 19/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 20/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 21/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 22/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 23/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 24/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 25/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 26/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 27/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 28/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 29/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 30/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 31/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 32/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 33/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 34/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 35/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 36/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 37/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 38/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 39/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 40/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 41/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 42/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 43/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 44/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 45/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 46/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 47/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 48/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 49/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 50/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 51/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 52/52 uploaded successfully to MySQL table: HealthDeficiencies\n",
      "Chunk 1/3 uploaded successfully to MySQL table: ProviderInfo\n",
      "Chunk 2/3 uploaded successfully to MySQL table: ProviderInfo\n",
      "Chunk 3/3 uploaded successfully to MySQL table: ProviderInfo\n",
      "Chunk 1/3 uploaded successfully to MySQL table: CostReports\n",
      "Chunk 2/3 uploaded successfully to MySQL table: CostReports\n",
      "Chunk 3/3 uploaded successfully to MySQL table: CostReports\n",
      "Chunk 1/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 2/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 3/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 4/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 5/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 6/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 7/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 8/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 9/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 10/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 11/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 12/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 13/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 14/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 15/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 16/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 17/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 18/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 19/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 20/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 21/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 22/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 23/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 24/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 25/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 26/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 27/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 28/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 29/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 30/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 31/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 32/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 33/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 34/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 35/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 36/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 37/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 38/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 39/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 40/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 41/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 42/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 43/43 uploaded successfully to MySQL table: QualityMsrMDS\n",
      "Chunk 1/1 uploaded successfully to MySQL table: CovidVax\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# MySQL connection parameters\n",
    "host= 'localhost'\n",
    "user= 'josh'\n",
    "password= 'go$T4GS'\n",
    "database= 'data_4999'\n",
    "\n",
    "# Create MySQL connection\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{user}:{password}@{host}/{database}\")\n",
    "\n",
    "# Trans\n",
    "joined_dfs = extract_transform(data_directory)\n",
    "\n",
    "# Iterate over the dictionary of DataFrames and upload each DataFrame to MySQL\n",
    "for table_name, df in joined_dfs.items():\n",
    "    upload_dataframe_to_mysql(df, table_name, engine)\n",
    "\n",
    "# Close the connection\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
